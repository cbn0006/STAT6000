---
title: "Chapter 6"
author: "Cody Nichols"
date: "`r Sys.Date()`"
output: html_document
---

# Cody Nichols' Chapter 6 Questions

### 8a

Done

```{r}
set.seed(12)

n <- 100

X <- rnorm(n)
epsilon <- rnorm(n)
```

### 8b

Done

```{r}
beta0 <- 3
beta1 <- 2
beta2 <- -3
beta3 <- 0.5

Y <- beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + epsilon
```

### 8c

X3 according to Cp, BIC, and R^2.Done

```{r}
library(leaps)

data <- data.frame(Y = Y)
for (i in 1:10) {
  data[[paste0("X", i)]] <- X^i
}
```


```{r}
best_subset <- regsubsets(Y ~ ., data = data, nvmax = 10)

reg_summary <- summary(best_subset)
reg_summary
```


```{r}
min_cp_model <- which.min(reg_summary$cp)
min_bic_model <- which.min(reg_summary$bic)
max_adjr2_model <- which.max(reg_summary$adjr2)
```


```{r}
par(mfrow = c(1, 3))

plot(reg_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "b", pch = 19)
points(min_cp_model, reg_summary$cp[min_cp_model], col = "red", cex = 2, pch = 20)

plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "b", pch = 19)
points(min_bic_model, reg_summary$bic[min_bic_model], col = "red", cex = 2, pch = 20)

plot(reg_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted R^2", type = "b", pch = 19)
points(max_adjr2_model, reg_summary$adjr2[max_adjr2_model], col = "red", cex = 2, pch = 20)
```


```{r}
coefficients_cp <- coef(best_subset, min_cp_model)
coefficients_bic <- coef(best_subset, min_bic_model)
coefficients_adjr2 <- coef(best_subset, max_adjr2_model)

print("Coefficients of the best model according to Cp:")
print(coefficients_cp)

print("Coefficients of the best model according to BIC:")
print(coefficients_bic)

print("Coefficients of the best model according to Adjusted R^2:")
print(coefficients_adjr2)
```

### 8d

Forward has the same results as 8c, while backward does not.

```{r}
forward_subset <- regsubsets(Y ~ ., data = data, nvmax = 10, method = "forward")
forward_summary <- summary(forward_subset)
forward_summary
```


```{r}
backward_subset <- regsubsets(Y ~ ., data = data, nvmax = 10, method = "backward")
backward_summary <- summary(backward_subset)
backward_summary
```

```{r}
min_cp_model_forward <- which.min(forward_summary$cp)
min_bic_model_forward <- which.min(forward_summary$bic)
max_adjr2_model_forward <- which.max(forward_summary$adjr2)

min_cp_model_backward <- which.min(backward_summary$cp)
min_bic_model_backward <- which.min(backward_summary$bic)
max_adjr2_model_backward <- which.max(backward_summary$adjr2)

identical(min_cp_model, min_cp_model_forward)
identical(min_cp_model, min_cp_model_backward)
```

### 8e

X1, X2, and X3 are the significant predictors with the predictors being very close to what they are expected to be.

```{r}
library(glmnet)

X_matrix <- model.matrix(Y ~ . - 1, data = data)
```


```{r}
set.seed(12)
cv_lasso <- cv.glmnet(X_matrix, Y, alpha = 1)

plot(cv_lasso)
```


```{r}
optimal_lambda <- cv_lasso$lambda.min

cat("Optimal lambda value:", optimal_lambda, "\n")

lasso_coefficients <- predict(cv_lasso, s = optimal_lambda, type = "coefficients")

print("Lasso Coefficient Estimates at Optimal Lambda:")
print(lasso_coefficients)
```

### 8f

Lasso identifies x7 as the most significant predictor with x5 as the only other non-zero predictor (noise). Best subset is obviously chosen to be x7 only.

```{r}
beta0_new <- 3
beta7 <- 7
Y_new <- beta0_new + beta7 * X^7 + epsilon

data$Y <- Y_new
```


```{r}
best_subset_new <- regsubsets(Y ~ ., data = data, nvmax = 10)
reg_summary_new <- summary(best_subset_new)

min_cp_model_new <- which.min(reg_summary_new$cp)
coefficients_cp_new <- coef(best_subset_new, min_cp_model_new)

print("Best Subset Selection Coefficients with New Y:")
print(coefficients_cp_new)
```


```{r}
Y_new_vector <- Y_new

cv_lasso_new <- cv.glmnet(X_matrix, Y_new_vector, alpha = 1)

optimal_lambda_new <- cv_lasso_new$lambda.min

lasso_coefficients_new <- predict(cv_lasso_new, s = optimal_lambda_new, type = "coefficients")

print("Lasso Coefficient Estimates with New Y:")
print(lasso_coefficients_new)
```

### 9a

Done

```{r}
library(ISLR)
library(glmnet)
library(pls)
library(MASS)

set.seed(12)

data("College")

head(College)

mean_apps <- mean(College$Apps, na.rm = TRUE)
cat("Mean of the Apps column:", mean_apps, "\n")
```


```{r}
n <- nrow(College)

train_indices <- sample(1:n, size = round(0.7 * n))

train_data <- College[train_indices, ]
test_data <- College[-train_indices, ]
```

### 9b

Done

```{r}
lm_fit <- lm(Apps ~ ., data = train_data)

summary(lm_fit)
```

```{r}
lm_pred <- predict(lm_fit, newdata = test_data)

lm_mse <- mean((test_data$Apps - lm_pred)^2)

cat("Test MSE for Linear Model:", lm_mse, "\n")
```

### 9c

Done

```{r}
x_train <- model.matrix(Apps ~ ., data = train_data)[, -1]
y_train <- train_data$Apps

x_test <- model.matrix(Apps ~ ., data = test_data)[, -1]
y_test <- test_data$Apps
```


```{r}
lambda_grid <- 10^seq(10, -2, length = 100)

set.seed(12)
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambda_grid, standardize = TRUE)

optimal_lambda_ridge <- cv_ridge$lambda.min

plot(cv_ridge)
```


```{r}
ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = optimal_lambda_ridge, standardize = TRUE)

ridge_pred <- predict(ridge_mod, s = optimal_lambda_ridge, newx = x_test)

ridge_mse <- mean((y_test - ridge_pred)^2)

cat("Test MSE for Ridge Regression Model:", ridge_mse, "\n")
```

### 9d

Done

```{r}
set.seed(12)
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda_grid, standardize = TRUE)

optimal_lambda_lasso <- cv_lasso$lambda.min

plot(cv_lasso)
```


```{r}
lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = optimal_lambda_lasso, standardize = TRUE)

lasso_pred <- predict(lasso_mod, s = optimal_lambda_lasso, newx = x_test)

lasso_mse <- mean((y_test - lasso_pred)^2)

cat("Test MSE for Lasso Model:", lasso_mse, "\n")

lasso_coef <- predict(lasso_mod, type = "coefficients", s = optimal_lambda_lasso)[1:18, ]

non_zero_coef <- sum(lasso_coef != 0)

cat("Number of non-zero coefficients in Lasso Model:", non_zero_coef, "\n")
```

### 9e

Done

```{r}
set.seed(12)
pcr_fit <- pcr(Apps ~ ., data = train_data, scale = TRUE, validation = "CV")

summary(pcr_fit)
```


```{r}
validationplot(pcr_fit, val.type = "MSEP")

optimal_M_pcr <- which.min(pcr_fit$validation$PRESS)

cat("Optimal number of components in PCR Model:", optimal_M_pcr, "\n")
```


```{r}
pcr_pred <- predict(pcr_fit, test_data, ncomp = optimal_M_pcr)

pcr_mse <- mean((y_test - pcr_pred)^2)

cat("Test MSE for PCR Model:", pcr_mse, "\n")
```

### 9f

Done

```{r}
set.seed(12)
pls_fit <- plsr(Apps ~ ., data = train_data, scale = TRUE, validation = "CV")

summary(pls_fit)
```


```{r}
validationplot(pls_fit, val.type = "MSEP")

optimal_M_pls <- which.min(pls_fit$validation$PRESS)

cat("Optimal number of components in PLS Model:", optimal_M_pls, "\n")
```

```{r}
pls_pred <- predict(pls_fit, test_data, ncomp = optimal_M_pls)

pls_mse <- mean((y_test - pls_pred)^2)

cat("Test MSE for PLS Model:", pls_mse, "\n")
```

### 9g

There is not much difference between all five approaches with Least Squares and PCR being the best and Lasso being the worst. We cannot accurately predict applicants because colleges usually receive an average of 3000 applications and our MSE allows us to give or take roughly 1300 applications on each prediction.

```{r}
test_errors <- data.frame(
  Method = c("Least Squares", "Ridge Regression", "Lasso Regression", "PCR", "PLS"),
  Test_MSE = c(lm_mse, ridge_mse, lasso_mse, pcr_mse, pls_mse)
)

print(test_errors)
```

### 11a

Done

```{r}
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(caret)

set.seed(12)

data("Boston")

head(Boston)
mean_crim <- mean(Boston$crim, na.rm = TRUE)
cat("Mean of the crim column:", mean_crim, "\n")
```


```{r}
train_indices <- sample(1:nrow(Boston), size = 0.7 * nrow(Boston))

train_data <- Boston[train_indices, ]
test_data <- Boston[-train_indices, ]

x_train <- model.matrix(crim ~ . - 1, data = train_data)
y_train <- train_data$crim

x_test <- model.matrix(crim ~ . - 1, data = test_data)
y_test <- test_data$crim
```

```{r}
best_subset <- regsubsets(crim ~ ., data = train_data, nvmax = 13)
best_summary <- summary(best_subset)

k <- 10
set.seed(1)
folds <- sample(1:k, nrow(train_data), replace = TRUE)
cv_errors <- matrix(NA, k, 13)

for (j in 1:k) {
  fold_train <- train_data[folds != j, ]
  fold_val <- train_data[folds == j, ]
  
  for (i in 1:13) {
    reg_fit <- regsubsets(crim ~ ., data = fold_train, nvmax = 13)
    coef_i <- coef(reg_fit, id = i)
    
    pred <- as.matrix(fold_val[, names(coef_i)[-1]]) %*% coef_i[-1] + coef_i[1]
    
    cv_errors[j, i] <- mean((fold_val$crim - pred)^2)
  }
}

mean_cv_errors <- colMeans(cv_errors)

optimal_model_size <- which.min(mean_cv_errors)

cat("Optimal number of predictors:", optimal_model_size, "\n")
```


```{r}
final_best_subset <- regsubsets(crim ~ ., data = train_data, nvmax = optimal_model_size)
final_coef <- coef(final_best_subset, id = optimal_model_size)
selected_variables <- names(final_coef)[-1]

cat("Selected variables in the best subset model:", selected_variables, "\n")
```


```{r}
x_test_best <- x_test[, selected_variables]

best_subset_pred <- as.matrix(x_test_best) %*% final_coef[-1] + final_coef[1]

best_subset_mse <- mean((y_test - best_subset_pred)^2)
cat("Test MSE for Best Subset Selection:", best_subset_mse, "\n")
```

```{r}
lambda_grid <- 10^seq(4, -2, length = 100)

set.seed(1)
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambda_grid, standardize = TRUE)

plot(cv_ridge)

optimal_lambda_ridge <- cv_ridge$lambda.min
cat("Optimal lambda for Ridge Regression:", optimal_lambda_ridge, "\n")
```


```{r}
ridge_model <- glmnet(x_train, y_train, alpha = 0, lambda = optimal_lambda_ridge, standardize = TRUE)

ridge_pred <- predict(ridge_model, s = optimal_lambda_ridge, newx = x_test)

ridge_mse <- mean((y_test - ridge_pred)^2)
cat("Test MSE for Ridge Regression:", ridge_mse, "\n")
```


```{r}
set.seed(12)
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda_grid, standardize = TRUE)

plot(cv_lasso)

optimal_lambda_lasso <- cv_lasso$lambda.min
cat("Optimal lambda for Lasso Regression:", optimal_lambda_lasso, "\n")
```


```{r}
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = optimal_lambda_lasso, standardize = TRUE)

lasso_pred <- predict(lasso_model, s = optimal_lambda_lasso, newx = x_test)

lasso_mse <- mean((y_test - lasso_pred)^2)
cat("Test MSE for Lasso Regression:", lasso_mse, "\n")

lasso_coef <- coef(lasso_model, s = optimal_lambda_lasso)

coefficients <- as.numeric(lasso_coef)

variable_names <- rownames(lasso_coef)

coefficients <- coefficients[-1]
variable_names <- variable_names[-1]

non_zero_indices <- which(coefficients != 0)
```


```{r}
selected_variables_lasso <- variable_names[non_zero_indices]

non_zero_coef <- length(non_zero_indices)

cat("Number of non-zero coefficients in Lasso Regression:", non_zero_coef, "\n")
cat("Variables selected by Lasso Regression:", selected_variables_lasso, "\n")
```

```{r}
set.seed(12)
pcr_model <- pcr(crim ~ ., data = train_data, scale = TRUE, validation = "CV")

summary(pcr_model)

validationplot(pcr_model, val.type = "MSEP")
```


```{r}
optimal_components <- which.min(pcr_model$validation$PRESS)
cat("Optimal number of components in PCR:", optimal_components, "\n")
```


```{r}
pcr_pred <- predict(pcr_model, test_data, ncomp = optimal_components)

pcr_mse <- mean((y_test - pcr_pred)^2)
cat("Test MSE for PCR:", pcr_mse, "\n")
```


### 11b

All models don't perform well/reasonably well on this dataset because they all have roughly the same MSE and the predicted value(crim) has an average of ~3. This means that these models are off by roughly 100% on average.

```{r}
test_mse_results <- data.frame(
  Method = c("Best Subset Selection", "Ridge Regression", "Lasso Regression", "PCR"),
  Test_MSE = c(best_subset_mse, ridge_mse, lasso_mse, pcr_mse)
)

print(test_mse_results)
```

### 11c

No, because less important variables don't need to be used in this prediction/regression task.

```{r}
print(selected_variables_lasso)
```

